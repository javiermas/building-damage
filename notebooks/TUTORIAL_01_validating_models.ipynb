{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 01: Validating models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we can see how to validate models. Note that it requires to have run the TUTORIAL_00 notebook first so we precompute the features that will be used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import argparse\n",
    "import json\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.errors import ResourceExhaustedError\n",
    "\n",
    "from damage.data import DataStream\n",
    "from damage.models import CNN, RandomSearch, CNNPreTrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '5'\n",
    "RESULTS_PATH = '../logs/experiments'\n",
    "FEATURES_PATH = '../logs/features'\n",
    "features_file_name = 'test_daraa.p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>annotation_date</th>\n",
       "      <th>damage_num</th>\n",
       "      <th>destroyed</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>city</th>\n",
       "      <th>patch_id</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">daraa</th>\n",
       "      <th>10080-8224</th>\n",
       "      <th>2017-02-07</th>\n",
       "      <td>2016-04-19</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.616861</td>\n",
       "      <td>36.122191</td>\n",
       "      <td>[[[99, 85, 74, 90, 65, 58], [99, 81, 74, 90, 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10208-8288</th>\n",
       "      <th>2017-02-07</th>\n",
       "      <td>2016-04-19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.616517</td>\n",
       "      <td>36.122878</td>\n",
       "      <td>[[[123, 125, 123, 90, 61, 58], [107, 113, 115,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10272-4768</th>\n",
       "      <th>2017-02-07</th>\n",
       "      <td>2016-04-19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.635400</td>\n",
       "      <td>36.123221</td>\n",
       "      <td>[[[99, 73, 66, 41, 40, 41], [99, 73, 66, 41, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10336-7904</th>\n",
       "      <th>2017-02-07</th>\n",
       "      <td>2016-04-19</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.618577</td>\n",
       "      <td>36.123565</td>\n",
       "      <td>[[[181, 190, 197, 132, 89, 74], [181, 186, 189...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10400-8160</th>\n",
       "      <th>2017-02-07</th>\n",
       "      <td>2016-04-19</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.617204</td>\n",
       "      <td>36.123908</td>\n",
       "      <td>[[[99, 97, 99, 49, 45, 41], [82, 85, 82, 49, 4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            annotation_date  damage_num  destroyed   latitude  \\\n",
       "city  patch_id   date                                                           \n",
       "daraa 10080-8224 2017-02-07      2016-04-19         2.0        0.0  32.616861   \n",
       "      10208-8288 2017-02-07      2016-04-19         1.0        0.0  32.616517   \n",
       "      10272-4768 2017-02-07      2016-04-19         1.0        0.0  32.635400   \n",
       "      10336-7904 2017-02-07      2016-04-19         3.0        1.0  32.618577   \n",
       "      10400-8160 2017-02-07      2016-04-19         2.0        0.0  32.617204   \n",
       "\n",
       "                             longitude  \\\n",
       "city  patch_id   date                    \n",
       "daraa 10080-8224 2017-02-07  36.122191   \n",
       "      10208-8288 2017-02-07  36.122878   \n",
       "      10272-4768 2017-02-07  36.123221   \n",
       "      10336-7904 2017-02-07  36.123565   \n",
       "      10400-8160 2017-02-07  36.123908   \n",
       "\n",
       "                                                                         image  \n",
       "city  patch_id   date                                                           \n",
       "daraa 10080-8224 2017-02-07  [[[99, 85, 74, 90, 65, 58], [99, 81, 74, 90, 6...  \n",
       "      10208-8288 2017-02-07  [[[123, 125, 123, 90, 61, 58], [107, 113, 115,...  \n",
       "      10272-4768 2017-02-07  [[[99, 73, 66, 41, 40, 41], [99, 73, 66, 41, 4...  \n",
       "      10336-7904 2017-02-07  [[[181, 190, 197, 132, 89, 74], [181, 186, 189...  \n",
       "      10400-8160 2017-02-07  [[[99, 97, 99, 49, 45, 41], [82, 85, 82, 49, 4...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = pd.read_pickle('{}/{}'.format(FEATURES_PATH, features_file_name)).dropna(subset=['destroyed'])\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "830"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make use of three custom classes: __RandomSearch__, __CNN__ and __DataStream__. __RandomSearch__ is a class that samples hyperparameters for ML models. As of may 2019, only the space for cnn's has been implemented. __CNN__ is a class that defines a Convolutional Neural Network model and follows the standards of Sklearn and Keras APIs, containing methods called fit, predict, fit_generator, predict_generator and validate_generator. In this case, we make use of the validate_generator method, which takes a generator of data as required by Keras's fit_generator method: each batch yields a tuple of (features, target). We use the __DataStream__ object to create these generators, first by generating the indices with the split_by_path_id method, which follows the standards of Sklearn splitters and then with the get_data_generator_from_index method that turns those indices into data generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0723 18:35:32.917455 4541539776 deprecation.py:323] From /Users/jordi/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:410: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dense_units': 128, 'batch_size': 33, 'convolutional_layers': [{'kernel_size': [7, 7], 'pool_size': [6, 6], 'filters': 32, 'dropout': 0.33333333333333337, 'activation': 'relu'}, {'kernel_size': [7, 7], 'pool_size': [6, 6], 'filters': 64, 'dropout': 0.33333333333333337, 'activation': 'relu'}, {'kernel_size': [7, 7], 'pool_size': [6, 6], 'filters': 128, 'dropout': 0.33333333333333337, 'activation': 'relu'}], 'epochs': 6, 'layer_type': 'cnn', 'class_weight': 1.15, 'learning_rate': 0.0017575106248547913}\n"
     ]
    }
   ],
   "source": [
    "# Modelling\n",
    "sampler = RandomSearch()\n",
    "models = {\n",
    "    CNN:sampler.sample_cnn,\n",
    "    CNNPreTrained: sampler.sample_cnn_pretrained,\n",
    "}\n",
    "Model = random.choice([CNN])\n",
    "sample_func = models[Model]\n",
    "spaces = sample_func(1)\n",
    "# # Do splits\n",
    "# class_proportion = {\n",
    "#     1: 0.3,\n",
    "# }\n",
    "batch_size = spaces[0]['batch_size']\n",
    "test_batch_size = batch_size\n",
    "train_proportion = 0.7\n",
    "data_stream = DataStream(batch_size = batch_size, train_proportion=train_proportion, test_batch_size=test_batch_size)\n",
    "unique_patches = features.index.get_level_values('patch_id').unique().tolist()\n",
    "train_patches = random.sample(unique_patches, round(len(unique_patches)*train_proportion))\n",
    "train_data = features.loc[features.index.get_level_values('patch_id').isin(train_patches)]\n",
    "# if train_data['destroyed'].mean() > class_proportion[1]:\n",
    "#     train_data_upsampled = train_data.copy()\n",
    "# else:\n",
    "#     train_data_upsampled = data_stream._upsample_class_proportion(train_data, class_proportion).sample(frac=1)\n",
    "test_patches = list(set(unique_patches) - set(train_patches))\n",
    "test_data = features.loc[features.index.get_level_values('patch_id').isin(test_patches)]\n",
    "\n",
    "train_indices = data_stream._get_index_generator(train_data, batch_size)\n",
    "test_indices = data_stream._get_index_generator(test_data, test_batch_size)\n",
    "train_generator = data_stream.get_train_data_generator_from_index(\n",
    "    [train_data['image'], train_data['destroyed']], train_indices)\n",
    "test_generator = data_stream.get_train_data_generator_from_index(\n",
    "    [test_data['image'], test_data['destroyed']], test_indices)\n",
    "train_dataset = Dataset.from_generator(lambda: train_generator, (tf.float32, tf.int32))\n",
    "test_dataset = Dataset.from_generator(lambda: test_generator, (tf.float32, tf.int32))\n",
    "num_batches = len(train_indices)\n",
    "num_batches_test = len(test_indices)\n",
    "print(spaces[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validate\n",
    "for space in spaces:\n",
    "    print('Now validating:\\n')\n",
    "    print(space)\n",
    "    try:\n",
    "        model = Model(**space)\n",
    "        losses = model.validate_generator(train_dataset, test_dataset,\n",
    "                                          steps_per_epoch=num_batches,\n",
    "                                          validation_steps=num_batches_test,\n",
    "                                          **space)\n",
    "    except Exception as e:\n",
    "        losses = {'log': str(e)}\n",
    "\n",
    "    losses['model'] = str(Model)\n",
    "    losses['space'] = space\n",
    "    losses['features'] = features_file_name\n",
    "    losses['num_batches_train'] = num_batches\n",
    "    losses['num_batches_test'] = num_batches_test\n",
    "    identifier = round(time())\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('{}/experiment_{}.json'.format(RESULTS_PATH, identifier), 'w') as f:\n",
    "        json.dump(str(losses), f)\n",
    "if 'val_recall_positives' in losses.keys():\n",
    "    if losses['val_recall_positives'][-1] > 0.4 and losses['val_precision_positives'][-1] > 0.1:\n",
    "        model.save('../logs/models/model_{}.h5'.format(identifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
