{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 00: Computing features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we can see how to compute features making use of the Pipeline and Feature classes. First, we import a function to load data from multiple cities and the module that contains the code for features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from damage.data import load_data_multiple_cities\n",
    "from damage import features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we start by reading the data for any cities we are interested in. In this case, we will load data from Aleppo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading\n",
    "cities = ['daraa']\n",
    "\n",
    "rasters_path = '../data/city_rasters/'\n",
    "annotations_path = '../data/annotations/'\n",
    "polygons_path = '../data/polygons/'\n",
    "\n",
    "data = load_data_multiple_cities(cities, rasters_path, annotations_path, polygons_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting object is a dictionary with the filenames preceded by a keyword (populated_areas, annotation, raster) as keys and data as values (dataframes, tifs, arrays...). By adding that keyword as a prefix we will be able to detect on a later step what type of data each key contains. \n",
    "\n",
    "The mapping between the city name (Aleppo in this case) and the corresponding filenames is done at damage/data/data_sources.py Ideally, we should create a standard way of naming the files so that file is not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['populated_areas_populated_areas.shp', 'annotation_4_Damage_Sites_Daraa_CDA.shp', 'raster_daraa_2011_10_17_zoom_19.tif', 'raster_daraa_2013_11_10_zoom_19.tif', 'raster_daraa_2014_05_01_zoom_19.tif', 'raster_daraa_2016_02_25_zoom_19.tif', 'raster_daraa_2016_04_19_zoom_19.tif', 'raster_daraa_2017_02_07_zoom_19.tif', 'no_analysis_areas_5_No_Analysis_Areas_Daraa.shp'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2013-09-07' None]\n",
      "['2014-05-01' None]\n",
      "['2015-06-04' None]\n",
      "['2016-04-19']\n"
     ]
    }
   ],
   "source": [
    "# check unique dates\n",
    "shapefile_df = data['annotation_4_Damage_Sites_Daraa_CDA.shp']\n",
    "print(shapefile_df.SensDt.unique())\n",
    "print(shapefile_df.SensDt_2.unique())\n",
    "print(shapefile_df.SensDt_3.unique())\n",
    "print(shapefile_df.SensDt_4.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get into the code of the Pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The feature pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ___Pipeline___ class takes a __list of tuples__ (name, function) for preprocessors and features and applies them with the transform method. __The feature functions need to return a dataframe with an identically structured index so the merge can be performed (e.g. city, patch_id).__\n",
    "\n",
    "The ___transform___ method __takes a dictionary of data__ where each key represents a different data source (e.g. annotations) and each value a data object (e.g. pandas dataframe). The transform         method __iterates first over the preprocessor functions, overwriting the data object__. Then, it __iterates over the feature functions creating new keys__ in the data dictionary with the passed name. That way, features can use data generates by previously computed features.\n",
    "\n",
    "Finally, the transform method merges the data generates by the feature functions making use of the __common index structure__.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import pandas as pd\n",
    "\n",
    "from damage.features.base import Transformer\n",
    "\n",
    "\n",
    "class Pipeline(Transformer):\n",
    "\n",
    "    def __init__(self, features, preprocessors):\n",
    "        self.features = features\n",
    "        self.feature_names = [feature_name for feature_name, _ in self.features]\n",
    "        self.preprocessors = preprocessors\n",
    "\n",
    "    def transform(self, data):\n",
    "        for preprocessor_name, preprocessor in self.preprocessors:\n",
    "            data = preprocessor(data)\n",
    "\n",
    "        for feature_name, feature in self.features:\n",
    "            data[feature_name] = feature(data)\n",
    "\n",
    "        feature_data = [data[name] for name in self.feature_names if name in data.keys()]\n",
    "        feature_data = self._merge_feature_data(feature_data)\n",
    "        return feature_data\n",
    "\n",
    "    def _merge_feature_data(self, feature_data):\n",
    "        return reduce(lambda l, r: pd.merge(l, r, left_index=True, right_index=True, how='outer'), feature_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A toy example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go first with a toy example. First we create our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>destroyed</th>\n",
       "      <th>patch_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   destroyed patch_id\n",
       "0          0        1\n",
       "1          1        2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations = pd.DataFrame({\n",
    "    'destroyed': [0, 1],\n",
    "    'patch_id': ['1', '2'],\n",
    "})\n",
    "annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>patch_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>image_a</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>image_b</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     image patch_id\n",
       "0  image_a        1\n",
       "1  image_b        2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = pd.DataFrame({\n",
    "    'image': ['image_a', 'image_b'],\n",
    "    'patch_id': ['1', '2'],\n",
    "})\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'annotations': annotations,\n",
    "    'image': image\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create some simple functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor_preprocess_annotations(data):\n",
    "    annotation_data = data['annotations']\n",
    "    data['annotations'] = annotation_data.rename(columns={'destroyed': 'damage'})\n",
    "    return data\n",
    "\n",
    "def feature_create_damage_dummy(data):\n",
    "    annotation_data = data['annotations'].set_index('patch_id')\n",
    "    damage_dummy = pd.get_dummies(annotation_data['damage'], drop_first=True, prefix='destroyed')\n",
    "    return damage_dummy\n",
    "\n",
    "def feature_split_images(data):\n",
    "    image_data = data['image'].set_index('patch_id')\n",
    "    # Split\n",
    "    return image_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply these functions to the data dictionary we created and we well get a pandas dataframe with our features, indexed by the common index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>destroyed_1</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>patch_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>image_a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>image_b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          destroyed_1    image\n",
       "patch_id                      \n",
       "1                   0  image_a\n",
       "2                   1  image_b"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline(\n",
    "    preprocessors=[\n",
    "        ('preprocess_annotations', preprocessor_preprocess_annotations)\n",
    "    ],\n",
    "    features=[\n",
    "        ('damage_dummy', feature_create_damage_dummy),\n",
    "        ('feature_split_images', feature_split_images)\n",
    "    ]\n",
    ")\n",
    "feature_data = pipeline.transform(data)\n",
    "feature_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A real example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we run a real example, with the data from daraa and some feature classes that exist in the damage library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading\n",
    "cities = ['daraa']\n",
    "\n",
    "rasters_path = '../data/city_rasters/'\n",
    "annotations_path = '../data/annotations/'\n",
    "polygons_path = '../data/polygons/'\n",
    "\n",
    "data = load_data_multiple_cities(cities, rasters_path, annotations_path, polygons_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:::AnnotationPreprocessor:::2019-07-26 17:17:38,427:::Applying AnnotationPreprocessor\n",
      "INFO:::RasterSplitter:::2019-07-26 17:17:38,748:::Applying RasterSplitter\n",
      "100%|██████████| 207/207 [01:59<00:00,  1.74it/s]\n",
      "100%|██████████| 207/207 [00:02<00:00, 69.69it/s]\n",
      "100%|██████████| 207/207 [00:01<00:00, 121.99it/s]\n",
      "100%|██████████| 207/207 [00:02<00:00, 72.77it/s]\n",
      "100%|██████████| 207/207 [00:01<00:00, 109.33it/s]\n",
      "100%|██████████| 207/207 [01:42<00:00,  2.01it/s]\n",
      "INFO:::AnnotationMaker:::2019-07-26 17:22:42,018:::Applying AnnotationMaker\n",
      "INFO:::RasterPairMaker:::2019-07-26 17:22:43,817:::Applying RasterPairMaker\n"
     ]
    }
   ],
   "source": [
    "### Processing\n",
    "from datetime import timedelta\n",
    "patch_size = 64\n",
    "stride = patch_size\n",
    "TIME_TO_ANNOTATION_THRESHOLD = timedelta(weeks=1)\n",
    "pipeline = features.Pipeline(\n",
    "    preprocessors=[\n",
    "        ('AnnotationPreprocessor', features.AnnotationPreprocessor()),\n",
    "    ],\n",
    "    features=[\n",
    "        ('RasterSplitter', features.RasterSplitter(patch_size=patch_size, stride=stride)),\n",
    "        ('AnnotationMaker', features.AnnotationMaker(patch_size, TIME_TO_ANNOTATION_THRESHOLD)),\n",
    "        ('RasterPairMaker', features.RasterPairMaker()),\n",
    "    ],\n",
    "\n",
    ")\n",
    "\n",
    "feature_data = pipeline.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15574\n",
      "14711\n",
      "DatetimeIndex(['2017-02-07'], dtype='datetime64[ns]', name='date', freq=None)\n"
     ]
    }
   ],
   "source": [
    "print(len(feature_data))\n",
    "print(feature_data['destroyed'].isnull().sum())\n",
    "print(feature_data.index.get_level_values('date').unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can save this data as a pickle file to retrieve it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data.to_pickle('../logs/features/example_daraa.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
